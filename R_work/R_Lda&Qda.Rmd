---
title: "Lda-Qda"
author: "Chase Enzweiler"
date: "10/30/2017"
output: html_document
---

### LDA and QDA Functions

\ 
\ 
```{r}
library(MASS)
library(mvtnorm)
```

\ 
\ 

### LDA

\ 
\ 

Implement an lda function that computes the necessary estimates for LDA

```{r}
# function for computing LDA estimates
##########################################################################
# input: X predictor matrix nxp, y response matrix a factor length n

# output: list(pi_hat : prior prob vector length K, mu_hat : Kxp matrix where each row contains mean of the group, sigma_hat : pxp covariance matrix of the predictors)

my_lda <- function(X, y){
  
  # number of groups/classes
  
  K <- length(levels(y))
  
  # number of observations
  n <- dim(X)[1]
  
  # number of predictors
  p <- dim(X)[2]
  
  # column bind response with predictors
  yX_mat <- cbind(y, X)
  
  # calculate the prior probability vector
  pi_hat <- c()
  
  for (i in 1:K){
    
    pi_hat[i] <- dim(yX_mat[yX_mat[,1] == levels(y)[i],])[1]
    
  }
  
  pi_hat <- (pi_hat / n)
  
  # calculate the group mean matrix k x p, where its group mean for each predictor
  
  mu_hat <- matrix(0, ncol = p, nrow = K)
  
  for (i in 1:K){
    
    mu_hat[i,] <- colMeans(yX_mat[yX_mat[,1] == levels(y)[i],-1])
    
  }
  
  rownames(mu_hat) <- levels(y)
  
  colnames(mu_hat) <- colnames(X)
  
  # calculate the pxp covariance matrix of predictors
  
  sums <- 0
  
  for (k in 1:K){
    
    # the x_i
    sub_mat <- as.matrix(yX_mat[yX_mat[,1] == levels(y)[k], -1] )
    
    first_sum <- 0
    
    for (i in 1:nrow(sub_mat)){
      
      first_sum <- first_sum + (sub_mat[i,] - mu_hat[k,]) %*% t( (sub_mat[i,]) - mu_hat[k,])
      
    }
    
    sums <- sums + first_sum
    
  }
  
  sigma_hat <- sums / (n - K)
  
  
  return(list("pi_hat" = pi_hat,"mu_hat" = mu_hat, "sigma_hat" =  sigma_hat, levels_order = levels(y)))
  
}

```
\ 
\ 

Implement a function called predict_my_lda() that generates predictions based on the output from my_lda()
\ 

```{r}
# function predict_my_lda
predict_my_lda <- function(fit, newdata){
  
  # input: fit output from my_lda(), newdata mxp matrix of new observations(assuming no response column)
  # output: list(class : length m factor vector each elements indicate the predicted class of observation, posterior : mxk matrix of posterior probabilities)
  
  # number of observations of newdata
  m <- dim(newdata)[1]
  
  # number of groups
  K <- length(fit$pi_hat)
  
  # calculate the posterior probabilities
  
  posterior <- matrix(0, nrow = m, ncol = K)
  
  for (i in 1:m){
    
    denominator <- 0
    
    for (k in 1:K){
      
      #bayes denominator 
      
      f_l <- dmvnorm(x = newdata[i,], mean = fit$mu_hat[k,], sigma = fit$sigma_hat)
      
      denominator <- denominator + (f_l * fit$pi_hat[k])
    }
    
    # now can calculate the posterior probabilities
    
    for (k in 1:K){
      
      posterior[i,k] <- (dmvnorm(x = newdata[i,], mean = fit$mu_hat[k,], sigma = fit$sigma_hat) * fit$pi_hat[k]) / denominator
      
      
    }
  
  }
  
  # posterior is matrix of probabilities for each observation that they would be in a given class. Classes are differentiated by column where columns are made by levels.
  colnames(posterior) <- fit$levels_order
  
  # class, make a length m factor vector of the predicted class
  
  class <- c()
  
  # find predicted class for each new observation
  for (i in 1:m){
    
    # find which col aka which class has highest posterior probability
    index <- which.max(posterior[i,])
    
    class[i] <- fit$levels_order[index]
  }

  return(list("posterior" = posterior, "predicted_class" = factor(class)))
  
}

```
\ 
\ 
Train your LDA on the first 140 observations of iris and predict the last 10 observations
\ 

```{r}
# train and predict on iris
my_lda_fit <- my_lda(iris[1:140, -5], iris$Species[1:140])

predict_my_lda(my_lda_fit, iris[141:150,-5])

# compare it to lda()

lda_fit <- lda(Species ~ ., data = iris[1:140,])

predict(lda_fit, newdata = iris[141:150,])

```
\ 
\ 

### QDA

\ 
\ 
Implement a function called my_qda() that computes the necessary estimates for QDA.

```{r}
# function for the qda estimates
# input: X: the predictor matrix, which is an n × p matrix
#– y: the response vector, which is a factor vector of length n

# output: list(– pi_hat: the prior probability vector, which is a vector of length K – mu_hat: a K × p matrix in which each row contains the mean of the group – sigma_hat: a p × p × K array, where sigma_hat[„k] contains the covariance)

my_qda <- function(X, y){
  
  # number of groups/classes
  
  K <- length(levels(y))
  
  # number of observations
  n <- dim(X)[1]
  
  # number of predictors
  p <- dim(X)[2]
  
  # column bind response with predictors
  yX_mat <- cbind(y, X)
  
  # calculate the prior probability vector
  pi_hat <- c()
  
  for (i in 1:K){
    
    pi_hat[i] <- dim(yX_mat[yX_mat[,1] == levels(y)[i],])[1]
    
  }
  
  pi_hat <- (pi_hat / n)
  
  # calculate the group mean matrix k x p, where its group mean for each predictor
  
  mu_hat <- matrix(0, ncol = p, nrow = K)
  
  for (i in 1:K){
    
    mu_hat[i,] <- colMeans(yX_mat[yX_mat[,1] == levels(y)[i],-1])
    
  }
  
  rownames(mu_hat) <- levels(y)
  
  colnames(mu_hat) <- colnames(X)
  
  # now find the p × p × K array, where sigma_hat[„k] contains the covariance matrix of the predictors in class k
  
  # array to store each groups covariance matrix
  sigma_hat <- array(0, dim = c(p,p,K), dimnames = list(colnames(X), colnames(X)))
  
  # observaations in group k
  n_k <- pi_hat * n
  
  # iterate through each group
  for (k in 1:K){
    
    # the x_i
    sub_mat <- as.matrix(yX_mat[yX_mat[,1] == levels(y)[k], -1] )
    
    # store the addition of covariance matrix for each obs.
    sums <- 0
    
    # iterate through each obs
    for (i in 1:nrow(sub_mat)){
      
       sums <- sums + (1/(n_k[k] - 1)) * (sub_mat[i,] - mu_hat[k,]) %*% t( (sub_mat[i,]) - mu_hat[k,])
      
    }
  
    
    sigma_hat[,,k] <- sums
    
  }
  
  
  return(list("pi_hat" = pi_hat, "mu_hat" = mu_hat, "sigma_hat" = sigma_hat, "levels_order" = levels(y)))
  
  
}

```
\ 
\ 

Implement a function called predict_my_qda() that generates predictions based on the output from my_qda().
\ 
```{r}
# function to generate predictions for my_qda function

# input
# – fit: the output from my_qda()
# – newdata: a m × p matrix of new observations

# output
# list(– class: a length-m factor vector; each of its elements indicate the predicted class of an observation, – posterior: a m × K matrix of posterior probabilities)

predict_my_qda <- function(fit, newdata){
  
  # number of observations of newdata
  m <- dim(newdata)[1]
  
  # number of groups
  K <- length(fit$pi_hat)
  
  # calculate the posterior probabilities
  
  posterior <- matrix(0, nrow = m, ncol = K)
  
  for (i in 1:m){
    
    denominator <- 0
    
    for (k in 1:K){
      
      #bayes denominator 
      
      f_l <- dmvnorm(x = newdata[i,], mean = fit$mu_hat[k,], sigma = fit$sigma_hat[,,k])
      
      denominator <- denominator + (f_l * fit$pi_hat[k])
    }
    
    # now can calculate the posterior probabilities
    
    for (k in 1:K){
      
      posterior[i,k] <- (dmvnorm(x = newdata[i,], mean = fit$mu_hat[k,], sigma = fit$sigma_hat[,,k]) * fit$pi_hat[k]) / denominator
      
      
    }
  
  }
  
  # posterior is matrix of probabilities for each observation that they would be in a given class. Classes are differentiated by column where columns are made by levels.
  colnames(posterior) <- fit$levels_order
  
  # class, make a length m factor vector of the predicted class
  
  class <- c()
  
  # find predicted class for each new observation
  for (i in 1:m){
    
    # find which col aka which class has highest posterior probability
    index <- which.max(posterior[i,])
    
    class[i] <- fit$levels_order[index]
  }

  return(list("posterior" = posterior, "predicted_class" = factor(class)))
  
}

```
\ 
\ 
Train your QDA on the first 140 observations in the dataset iris and predict the last 10 observations.

\ 
```{r}
# train and predict on iris
my_qda_fit <- my_qda(iris[1:140, -5], iris$Species[1:140])

predict_my_qda(my_qda_fit, iris[141:150, -5])

# confirm answers with qda()

qda_fit <- qda(Species ~ ., data = iris[1:140,])

predict(qda_fit, newdata = iris[141:150,])

```
\ 
\ 

### Confusion Matrix

\ 
```{r}
# training and test set
set.seed(100)
train_idx <- sample(nrow(iris), 90) 
train_set <- iris[train_idx, ] 
test_set <- iris[-train_idx, ]
```
\ 

Train LDA and QDA based on train_set

\ 
```{r}
# train lda and qda
lda_train <- my_lda(train_set[,-5], train_set$Species)

qda_train <- my_qda(train_set[,-5], train_set$Species)

```
\ 

Generate predictions on test_set

\ 
```{r}
# predict lda and qda
lda_predict <- predict_my_lda(lda_train, newdata = test_set[,-5])

qda_predict <- predict_my_qda(qda_train, newdata = test_set[,-5])
```
\ 

Compute the confusion matrix for each method.

\ 
```{r}
# confusion matrix

# lda confusion matrix
setosa_count <- dim(test_set[test_set[,5] == "setosa",])[1]

versicolor_count <- dim(test_set[test_set[,5] == "versicolor",])[1]

virginica_count <- dim(test_set[test_set[,5] == "virginica",])[1]

lda_set_count <- length(lda_predict$predicted_class[lda_predict$predicted_class == "setosa"])

lda_versi_count <- length(lda_predict$predicted_class[lda_predict$predicted_class == "versicolor"])

lda_virg_count <- length(lda_predict$predicted_class[lda_predict$predicted_class == "virginica"])


lda_confusion <- matrix(diag(c(lda_set_count, lda_versi_count, lda_virg_count, 0)), ncol = 4, nrow = 4, dimnames = list(c("True setosa", "True versicolor", "True virginica", "total"), c("predict setosa", "predict versicolor", "predict virginica", "total") ))

lda_confusion[,4] <- c(24, 17, 19, 60)
lda_confusion[4,] <- c(24, 18, 18, 60)

# qda confusion matrix

qda_set_count <- length(qda_predict$predicted_class[qda_predict$predicted_class == "setosa"])

qda_versi_count <- length(qda_predict$predicted_class[qda_predict$predicted_class == "versicolor"])

qda_virg_count <- length(qda_predict$predicted_class[qda_predict$predicted_class == "virginica"])

qda_confusion <- matrix(diag(c(qda_set_count, qda_versi_count, qda_virg_count, 0)), ncol = 4, nrow = 4, dimnames = list(c("True setosa", "True versicolor", "True virginica", "total"), c("predict setosa", "predict versicolor", "predict virginica", "total") ))

qda_confusion[,4] <- c(24, 17, 19, 60)
qda_confusion[4,] <- c(24, 18, 18, 60)

# LDA Confusion Matrix
data.frame(lda_confusion)

# QDA confusion Matrix
data.frame(qda_confusion)
```
















