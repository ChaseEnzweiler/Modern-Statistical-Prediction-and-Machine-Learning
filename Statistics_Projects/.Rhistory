pcr_fit <- pcr(training$lpsa ~ ., data = stand_train_pred, scale = FALSE, validation = "CV" )
summary(pcr_fit)
# matrix of the coefficients
pcr_coef_matrix <- matrix(pcr_fit$coefficients, nrow = 8, ncol = 8)
rownames(pcr_coef_matrix) <- colnames(stand_train_pred)
pcr_coef_matrix <- t(pcr_coef_matrix)
# profiles of coefficients plot
matplot(c(1,2,3,4,5,6,7,8), pcr_coef_matrix[,1:8], type = "s", lty = 1, col = c("red", "blue", "green", "purple", "yellow", "orange", "cyan", "pink"), xlab = "Number of Components", ylab = "Standardized Coefficients")
legend(1,.75, colnames(pcr_coef_matrix), cex = .45 , lty=c(1,1), lwd=c(1,1), col=c("red", "blue", "green", "purple", "yellow", "orange", "cyan", "pink"))
# plot cross validation mse against the number of components
validationplot(pcr_fit, val.type = "MSEP", ylab = "Cross-Validation MSE")
# optimal principal component regression model
# coefficients of optimal model
pcr_fit$coefficients[, , 8]
# number of components used in optimal model
pcr_fit$ncomp
# partial least squares regression
set.seed(1)
plsr_fit <- plsr(training$lpsa ~ ., data = stand_train_pred, scale = FALSE, validation = "CV")
summary(plsr_fit)
# plot of profile of coefficients
#matrix of coefficients
plsr_coef_matrix <- matrix(plsr_fit$coefficients, ncol = 8, nrow = 8)
rownames(plsr_coef_matrix) <- colnames(stand_train_pred)
plsr_coef_matrix <- t(plsr_coef_matrix)
# time to plot similiar to pcr
matplot(c(1,2,3,4,5,6,7,8), plsr_coef_matrix[,1:8], type = "s", lty = 1, col = c("red", "blue", "green", "purple", "yellow", "orange", "cyan", "pink"), xlab = "Number of Components", ylab = "Standardized Coefficients")
legend(1,.75, colnames(plsr_coef_matrix), cex = .45 , lty=c(1,1), lwd=c(1,1), col=c("red", "blue", "green", "purple", "yellow", "orange", "cyan", "pink"))
# now plot the MSE against the components
validationplot(plsr_fit, val.type = "MSEP")
# our optimal partial least squares model
plsr_fit$coefficients[, , 6]
# fit a ridge regression model
set.seed(1)
# create standardized design matrix to run in glmnet
stand_train_pred_mat <- as.matrix(stand_train_pred)
# 10 fold cross validation to find best lambda
cv_rr <- cv.glmnet(stand_train_pred_mat, training$lpsa, alpha = 0)
# lambda with smallest cross validation error
rr_lamda_best <- cv_rr$lambda.min
# fit the ridge regression model with the best lambda tuning parameter
rr_fit <- glmnet(stand_train_pred_mat, training$lpsa, alpha = 0, lambda = rr_lamda_best)
# plot of Cross Validation Mean squared error
plot(cv_rr)
# plot of profiles of coefficients using lambda and l1 norm
plot(glmnet(stand_train_pred_mat, training$lpsa, alpha = 0), xvar = "lambda")
plot(glmnet(stand_train_pred_mat, training$lpsa, alpha = 0))
# the Lasso
set.seed(1)
# find the lambda with the best cv mse for a tuning parameter
cv_lasso <- cv.glmnet(stand_train_pred_mat, training$lpsa, alpha = 1)
# best tuning parameter
lasso_lambda_best <- cv_lasso$lambda.min
# fit the lasso model with best best tuning paramter
lasso_fit <- glmnet(stand_train_pred_mat, training$lpsa, alpha = 1, lambda = lasso_lambda_best)
# plot of cross validation mse
plot(cv_lasso)
# plot of the profile of coefficients # use without best lambda
plot(glmnet(stand_train_pred_mat, training$lpsa, alpha = 1))
#optimal ridge regression
coef(rr_fit)
# best tuning parameter
rr_lamda_best
# optimal lasso model
coef(lasso_fit)
# best tuning parameter
lasso_lambda_best
# compute test MSE for ols model
# standardize the test data
stand_test <- as.data.frame(scale(test))
# number of observations in the test data
n <- dim(test)[1]
# predicted response values
lm_predicted <- predict(ols_fit, newdata = stand_test)
# test mse
lm_test_mse <- sum((test$lpsa - lm_predicted)^2) / n
lm_test_mse
# compute test mse for subset regression model
# prediction for our ols with 7
subset_predicted <- predict(lm(training$lpsa ~ lcavol + lweight, data = stand_train_pred), newdata = stand_test)
# test mse
subset_test_mse <- sum((test$lpsa - subset_predicted)^2) / n
subset_test_mse
# pcr test mse
pcr_predicted <- predict(pcr_fit, newdata = stand_test, ncomp = 8)
# test mse
pcr_test_mse <- sum((test$lpsa - pcr_predicted)^2) / n
pcr_test_mse
# plsr test mse
plsr_predicted <- predict(plsr_fit, newdata = stand_test, ncomp = 6 )
# test mse
plsr_test_mse <- mean((test$lpsa - plsr_predicted)^2)
plsr_test_mse
# ridge regression test mse
rr_predicted <- predict(rr_fit, newx = as.matrix(stand_test)[,-9], s = rr_lamda_best)
# test mse
rr_test_mse <- mean((test$lpsa - rr_predicted)^2)
rr_test_mse
# test mse for lasso
lasso_predicted <- predict(lasso_fit, s = lasso_lambda_best, newx = as.matrix(stand_test)[,-9])
# test mse
lasso_test_mse <- mean((test$lpsa - lasso_predicted)^2)
lasso_test_mse
# create a table
# putting together columns to put in a nice table
ols_col <- append(coef(ols_fit), lm_test_mse)
sub_col <- append(coef(subset_reg, 2), subset_test_mse)
sub_col <- append(sub_col, c(0, 0,0,0,0,0), after = 3)
rr_col <- append(as.vector(coef(rr_fit)), rr_test_mse)
las_col <- append(as.vector(coef(lasso_fit)), lasso_test_mse)
pcr_col <- append(0, coef(pcr_fit, 8))
plsr_col <- append(0, coef(plsr_fit, 6))
pcr_col <- append(pcr_col, pcr_test_mse)
plsr_col <- append(plsr_col, plsr_test_mse)
my_table <- cbind(ols_col, sub_col, rr_col, las_col, pcr_col, plsr_col)
rownames(my_table) <- c("Intercept", "lcavol", "lweight", "age", "lbph", "svi", "lcp", "gleason", "pgg45", "test mse")
colnames(my_table) <- c("LS", "Best Subset", "Ridge", "Lasso", "PCR", "PLSR")
# i dont have an intercept for PCR or PLSR
kable(round(my_table, digits = 4))
# packages
library(ElemStatLearn)
library(broom)
library(leaps)
library(pls)
library(caret)
library(ggplot2)
library(glmnet)
library(knitr)
# prostate data
data <- prostate
# training data
training <- data[data$train == TRUE,-10]
# test data
test <- data[data$train == FALSE, -10]
# matrix of correlations of predictors, -9 removes lpsa
cor_mat <- cor(training[,-9])
# matrix of correlations of predictors, -9 removes lpsa
cor_mat <- cor(training[,-9])
cor_mat
# matrix of correlations of predictors, -9 removes lpsa
cor_mat <- cor(training[,-9])
round(cor_mat)
# matrix of correlations of predictors, -9 removes lpsa
cor_mat <- cor(training[,-9])
round(cor_mat, 3)
# pairs plot
temperature <- read.csv("~/Desktop/stat 154/temperature.csv")
pairs(temperature[,2:13])
# stars plot
row_character_vect <- as.matrix(temperature[,1])
row_character_vect <- as.vector(row_character_vect)
stars(temperature[, 2:13], labels = row_character_vect, key.loc = c(-2,12), flip.labels = FALSE)
# summary of month temperatures
summary(temperature$January)
summary(temperature$March)
# boxplot
boxplot(temperature[,2:13])
# boxplot of standardized data
boxplot(scale(as.matrix(temperature[2:13])))
# variances
var(temperature$January)
var(temperature$February)
var(temperature$May)
# data of active individuals and variables
active_temp <- as.matrix(temperature[1:23, 2:13])
row.names(active_temp) <- row_character_vect[1:23]
# standardized active data
s_active_temp <- scale(active_temp, center = TRUE, scale = TRUE)
n <- dim(s_active_temp)[1]
p <- dim(s_active_temp)[2]
# calculate the sample correlation matrix
R <- (1/(n-1)) * t(s_active_temp) %*% s_active_temp
decomp_R <- eigen(R)
# matrix of eigenvectors(loadings)
V <- decomp_R$vectors
colnames(V) <- paste("v", 1:12, sep = "")
rownames(V) <- colnames(active_temp)
V[,1:4]
# has dimnsion p x r = 12 x 12
# note need to name rows dont know what to name them
# calculate principal components (scores) matrix
# has dim nxr = ind x rank
Z <- s_active_temp %*% V
colnames(Z) <- paste("PC", 1:12)
Z[,1:4]
e_value_vect <- decomp_R$values
e_value_vect
sum(e_value_vect)
# proportion of explained inertia (percentage)
proportion <- (e_value_vect / p) * 100
#cumulative proportion of explained inertia
cum_prop <- c(proportion[1])
for(i in 2:12){
cum_prop <- append(cum_prop, sum(proportion[1:i]))
}
# summary table of eigen values
eigen_sum_tab <- cbind(e_value_vect, proportion, cum_prop)
rownames(eigen_sum_tab) <- paste("comp", 1:12)
eigen_sum_tab <- as.table(eigen_sum_tab)
eigen_sum_tab
# create a scree-plot from scratch
plot(1:12, e_value_vect, ylab = "Eigenvalues", type = "b", xlab = "Number of Principal Components", main = "Scree-Plot")
# incorporate the supplementary cities
# we have to mean center and scale the supplementary cities according to our standardization of active cities
# we standardize using mean and sd of active cities
active_means <- colMeans(active_temp)
active_sd <- apply(active_temp, 2, sd)
supp_temp <- as.matrix(temperature[24:35 , 2:13])
row.names(supp_temp) <- row_character_vect[24:35]
# standardized with respect to active cities
s_supp_temp <- t(t(supp_temp) - active_means)
for(i in 1:12){
s_supp_temp[,i] <- s_supp_temp[,i] / active_sd[i]
}
library(ggplot2)
# supplemental projections
Z_s <- s_supp_temp %*% V
Z_df <- as.data.frame(Z)
Z_df$Area <- temperature[1:23, 18]
Z_s_df <- as.data.frame(Z_s)
Z_s_df$Area <- temperature[24:35, 18]
#added
colnames(Z_s_df) <- colnames(Z_df)
ggplot(data = Z_df, aes(x = `PC 1`, y = `PC 2`, group = Area)) + geom_point(aes(color = Area)) + geom_point(data = Z_s_df, aes(x = Z_s_df$`PC 1`, y = Z_s_df$`PC 2`, color = Area), shape = 3)
ggplot(data = rbind(Z_df, Z_s_df), aes(x = `PC 1`, y = `PC 2`, group = Area)) + geom_point(data = Z_df, aes(color = Area)) + geom_point(data = Z_s_df, aes(x = Z_s_df$`PC 1`, y = Z_s_df$`PC 2`, color = Area), shape = 3) + geom_text(aes(label = rownames(rbind(Z_df, Z_s_df))), hjust = 0, vjust = 0)
# we want matrix of Zik^2 / d^2(xi,g)
# the centroid of centered data is the origin
#since the centroid is zero the euclidean squared distance we just need to square the observations
dist2centroid <- apply(s_active_temp, 1, function(x) sum(x*x))
cos2_matrix <- sweep(Z^2, 1, dist2centroid, FUN = "/")
cos2_matrix[,1:4]
# ctr(i,k) = (mi * Zik^2)/lamda k
# percentage of inertia explained by individual i on component k
ctr <- (1 / (n - 1)) * Z^2
ctr <- t(t(ctr) / e_value_vect) * 100
ctr[,1:4]
# plot the contributions
ctr_df <- data.frame(ctr)
ctr_df$city <- 1:nrow(ctr_df)
ctr_df$zeros <- rep(0, nrow(ctr_df))
ggplot(data = ctr_df, aes(x = city, y = PC.1)) + geom_point() + geom_segment(aes(x = city, xend = city, y = zeros, yend = PC.1)) + scale_x_discrete(limit = rownames(ctr_df)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Contributions of individuals to PC1")
ggplot(data = ctr_df, aes(x = city, y = PC.2)) + geom_point() + geom_segment(aes(x = city, xend = city, y = zeros, yend = PC.2)) + scale_x_discrete(limit = rownames(ctr_df)) +
theme(axis.text.x = element_text(angle = 90, hjust = 1)) + ggtitle("Contributions of individuals to PC2")
# correlation matrix of variables with PC's
#standardized supplementary variables
supp_var <- temperature[1:23,14:17]
stand_supp_var <- scale(supp_var)
# I am going to include only the active individuals
# matrix of both active and supplementary var
s_both_temp <- cbind(s_active_temp, stand_supp_var)
var_pc_corr <- cor(s_both_temp, Z)
var_pc_corr[,1:4]
# circle of correlations
var_pc_corr <- as.data.frame(var_pc_corr)
radians <- seq(0, 2*pi, length = 100)
circle_frame <- data.frame(x = sin(radians), y = cos(radians))
ggplot(data = var_pc_corr, aes(`PC 1`, `PC 2`)) + geom_segment(data = var_pc_corr, aes(x = 0, y = 0, xend = `PC 1`, yend = `PC 2`), arrow = arrow(length = unit(0.2, "cm"))) + geom_polygon(aes(x,y), data = circle_frame, color = "black", fill = NA)  +
geom_text(data = var_pc_corr[1:12,], label = rownames(var_pc_corr[1:12,])) + geom_text(data = var_pc_corr[13:16,], label = rownames(var_pc_corr[13:16,]), color = "green")
ggplot(data = var_pc_corr, aes(`PC 1`, `PC 2`)) + geom_segment(data = var_pc_corr, aes(x = 0, y = 0, xend = `PC 1`, yend = `PC 2`), arrow = arrow(length = unit(0.2, "cm"))) + geom_polygon(aes(x,y), data = circle_frame, color = "black", fill = NA)
# load data and packages
library(glmnet)
library(nnet)
library(ggplot2)
wine_data <- read.csv("~/Desktop/stat 154/wine.data.csv")
# total sum of squares function
TSS <- function(x){
# x is input vector
return( sum( (x - mean(x))^2 ) )
}
# between groups sum of squares can also compute WSS
BSS <- function(x, y, WSS = FALSE){
# y is vector or factor for response
# x is vector for the predictor
# check vectors are same length
if (length(y) != length(x)){
stop("vectors must be same length")
}
# convert y to factor if it is not one
if (is.factor(y) != FALSE){
y <- as.factor(y)
}
# put the classes in terms of integers where each integer is a different   class factor(class1, classf) = 1, 2
y <- as.integer(y)
# take mean of x
x_bar <- mean(x)
# create a matrix with columns y and x
xy_mat <- cbind(y, x)
# store the sums
sums <- 0
if (WSS == FALSE){
# computes the formula for each class # BSS
for (i in unique(y)){
# observations in class
n <- length(xy_mat[xy_mat[,1] == i, 2])
# mean of observations in class
x_bar_k <- mean(xy_mat[xy_mat[,1] == i, 2])
sums <- sums + n * (x_bar_k - x_bar)^2
}
} else{
# computes WSS
for (i in unique(y)){
# group mean
x_bar_k <- mean(xy_mat[xy_mat[,1] == i, 2])
# WSS formula
sums <- sums + sum( (xy_mat[xy_mat[,1] == i, 2] - x_bar_k)^2)
}
}
return(sums)
}
# correlation ratio
cor_ratio <- function(x, y){
return(BSS(x, y) / TSS(x))
}
# F-ratio
F_ratio <- function(x, y){
# variable x
# response y
k <- nlevels(as.factor(y))
n <- length(x)
f <- (BSS(x, y)/(k - 1)) / (BSS(x, y, WSS = TRUE)/(n - k))
return(f)
}
# simple logistic regression
# however there are 3 classes so we can not use glm for our logistic regression, we will use multinom from package nnet.
########################################
# correction update to homework, now run logistic regression on the first two classes
#######################################
# create vector to store AIC values
AIC <- c()
# vector of predictor names
name_vector <- names(wine_data[,-1])
two_class <- wine_data[1:130,]
two_class$class <- two_class$class - 1
for (i in 1:length(name_vector)){
# length(name_vector) is 13 because it doesnt include class
# below wine_data[,i+1] so we dont use class as predictor
fit <- glm(class ~ two_class[,i+1], data = two_class, family = "binomial")
AIC[i] <- fit$aic
}
names(AIC) <- name_vector
# sort by increasing AIC
AIC <- sort(AIC)
# data frame of sorted AIC
AIC_table <- as.data.frame(AIC)
AIC_table
# barplot of sorted AIC
barplot(AIC, las = 2, ylab = "AIC", main = "AIC vs Predictors", col = "purple")
# correlations ratios
# store the ratios in this list
c_ratio <- c()
for (i in 1:length(name_vector)){
# i + 1 column so we dont include class
c_ratio[i] <- cor_ratio(wine_data[,i+1], wine_data[,1])
}
names(c_ratio) <- name_vector
# sort the ratios
c_ratio <- sort(c_ratio)
# ranked data frame of correlations
as.data.frame(c_ratio)
# barplot of ratios
barplot(c_ratio,las = 2, ylab = "Correlation Ratio", main = "Correlation Ratio Between Class and Preds", col = "green")
# F ratios
# store the F ratios in list
F_list <- c()
for (i in 1:length(name_vector)){
# i + 1 column so we dont include class
F_list[i] <- F_ratio(wine_data[,i+1], wine_data[,1])
}
names(F_list) <- name_vector
# sort by increasing F_list
F_list <- sort(F_list)
as.data.frame(F_list)
# barchart
barplot(F_list, las = 2, ylab = "F ratios", main = "F ratios vs Predictors", col = "cyan")
# function to create sample variance covariance matrix
total_variance <- function(X){
# X is matrix of predictors
# number of observations
n <- dim(X)[1]
# mean center but dont scale
X <- scale(X, center = TRUE, scale = FALSE)
return( (t(X) %*% X) / (n-1) )
}
total_variance(iris[,1:4])
# function for between variances
between_variance <- function(X, y){
# X is matrix of predictors
# y is response variable
# number of total observations
n <- dim(X)[1]
# convert y to classes referring to integers
y <- as.integer(as.factor(y))
# column binded matrix of x and y
xy_mat <- cbind(y, X)
# matrix to store sums of other matrices
# subtract one because we dont want to consider response
sum_mat <- 0
# centroid g of predictors
g <- colMeans(xy_mat[,-1])
# variances for each class k
for (i in unique(y)){
# filter the xy matrix by class to get a group
group_mat <- xy_mat[xy_mat[,1] == i, -1]
# observations in group
n_k <- dim(group_mat)[1]
# centroids of group
g_k <- colMeans(group_mat)
sum_mat <- sum_mat + (n_k / (n-1)) * (g_k - g) %*% t(g_k - g)
}
rownames(sum_mat) <- colnames(sum_mat)
return(sum_mat)
}
between_variance(iris[,1:4], iris$Species)
# within group variance
within_variance <- function(X, y){
# X is matrix of predictors
# y is response variable
n <- dim(X)[1]
# converts y to integer classes
y <- as.integer(as.factor(y))
xy_mat <- cbind(y, X)
sum <- 0
# loop through each group
for (i in unique(y)){
X_k <- as.matrix(xy_mat[xy_mat[,1] == i, -1])
n_k <- dim(X_k)[1]
W_k <- total_variance(X_k)
sum <- sum + ((n_k - 1)/(n - 1)) * W_k
}
return(sum)
}
within_variance(iris[,1:4], iris$Species)
# test our functions using iris data set
total_variance(iris[,1:4])
between_variance(iris[,1:4], iris$Species) + within_variance(iris[,1:4], iris$Species)
# find eigenvectors u_k
# within variance matrix W
W <- within_variance(wine_data[,-1], wine_data[,1])
# between variance matrix B
B <- between_variance(wine_data[,-1], wine_data[,1])
# decompose B into B = C %*% t(C)
# number of total observations in wine data
n <- dim(wine_data)[1]
# col means of predictors
xj_bar <- colMeans(wine_data[,-1])
# col means of predictors for each class
xjk_bar <- list()
# number of observations in each wine class
n_k <- c()
for (i in unique(wine_data$class)){
xjk_bar[[i]] <- colMeans(wine_data[wine_data[,1] == i, -1])
n_k[i] <- dim(wine_data[wine_data[,1] == i, -1])[1]
}
# create matrix C
C <- sqrt(n_k[1]/ (n-1)) * (xjk_bar[[1]] - xj_bar)
for (i in 2:length(unique(wine_data$class))){
new_row <- sqrt(n_k[i]/ (n-1)) * (xjk_bar[[i]] - xj_bar)
C <- cbind(C, new_row)
}
colnames(C) <- c("class 1", "class 2", "class 3")
# now we can use eigen value decomposition to find the eigenvectors w of t(C) %*% solve(W) %*% C
w <- eigen(t(C) %*% solve(W) %*% C)
# now recover the eigenvectors u with w, u = solve(W) %*% C %*% w
u <- solve(W) %*% C %*% w$vectors
# These are our eigenvectors u
u
# the u are the vectors associated with the canonical axes and we keep the minimum of k-1 and P, therfore we keep 2 canonical axes and to vectors of u
# obtain the linear combination z_k
z_k <- as.matrix(wine_data[,-1]) %*% u[,1:2]
# create a scatter plot
wine_lda <- data.frame(z_k)
colnames(wine_lda) <- c("LD1", "LD2")
wine_lda$class <- factor(wine_data$class)
ggplot(data = wine_lda, aes(LD1, LD2, color = class)) + geom_point()
# find the first two principal components
pca <- prcomp(wine_data[,-1], scale = TRUE)
# the principal components are contained in pca$rotation
# need to use on standardized data
wine_pca <- as.matrix(scale(wine_data[,-1])) %*% pca$rotation[,1:2]
wine_pca <- data.frame(wine_pca)
wine_pca$class <- factor(wine_data$class)
ggplot(data = wine_pca, aes(PC1, PC2, color = class)) + geom_point()
# calculate the correlations between z_k and the predictors
cor(z_k[,1], wine_data[,-1])
cor(z_k[,2], wine_data[,-1])
# n x k matrix of the squared mahalanobis distances
# centroids of each predictor with respect to the class g_k were calculated above as xjk_bar
# matrix of the mahalanobis distance
D2 <- matrix(0,nrow = dim(wine_data)[1] ,ncol = length(unique(wine_data$class)))
for (k in unique(wine_data$class)){
for (i in 1:dim(wine_data)[1]){
D2[i,k] <- (as.matrix(wine_data[i,-1]) - xjk_bar[[k]]) %*% solve(W) %*% t(as.matrix(wine_data[i,-1]) - xjk_bar[[k]])
}
}
# use which.min on each row of matrix, will give col with smallest distance, column corresponds the class
# predicted class for each observation according to mahalanobis distance
predicted_class <- c()
for (i in 1:dim(wine_data)[1]){
predicted_class[i] <- which.min(D2[i,])
}
# check to see how many observations were predicted correctly
wine_data$class == predicted_class
# all the observations were predicted into the true classes
# confusion matrix
confusion_mat <- diag(c(length(which(wine_data$class ==1)), length(which(wine_data$class ==2)), length(which(wine_data$class ==3))))
confusion_mat <- rbind(confusion_mat ,c(59, 71, 48))
confusion_mat <- cbind(confusion_mat ,c(59, 71, 48,(59+71+48)))
colnames(confusion_mat) <- c("True 1", "True 2", "True 3", "total")
rownames(confusion_mat) <- c("pred 1", "pred 2", "pred 3", "total")
confusion_mat <- data.frame(confusion_mat)
confusion_mat
